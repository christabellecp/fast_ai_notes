{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fast.ai notes\n",
    "\n",
    "### **KEY NOTES**\n",
    "\n",
    "#### **OBJECTS**\n",
    "1. ImageDataBunch = contains 2 or 3 data sets (training, validation and testing)\n",
    "2. ConvLearner = creates convolutional neural net (**inputs:** databunch, model/architecture (resnet34 and resnet50, number of layers **optional:** input metrics which prints something out like error rate )\n",
    "3.  Resenet34 was already trained looking at one and half million pictures looking at a thousand of different things. We download those pre-trained weights so we don't start with nothing. Uses ImageNet. \n",
    "4. ClassificationInterpretation.from_learner(convlearner) #see what comes out, interpretation of model\n",
    "5. Path object \n",
    "\n",
    "#### **FUNCTIONS**\n",
    "\n",
    "1. untar_data = download extract data from URL\n",
    "    input = URL\n",
    "    output=extracted data in a path\n",
    "2. help = shows how to use a function, gives type information\n",
    "3. get_image_files = grabs all image files in an array\n",
    "    input = image file\n",
    "    output = array of all image files\n",
    "4. transforms = makes something size 224 and center crops (crop and resize)\n",
    "5. normalize = makes all data about the same size (same mean and SD)\n",
    "13. docs(method class or function) = tells information \n",
    "\n",
    "\n",
    "##### DATABUNCH FUNCTIONS\n",
    "1. databunch.show_batch = shows some contents (images) in databunch\n",
    "    input: rows\n",
    "    output: contents of data bunch\n",
    "2. databunch.classes = shows all of the labels\n",
    "8. databunch.c = can think of as number of classes for classification problems not regression\n",
    "4. databunch.from_folder(path, ds_tfms=tfms, size=26) - creates a databunch if the labels of the data are the folder in which the files are in.\n",
    "\n",
    "##### CONVLEARNER FUNCTIONS\n",
    "1. ConvLearner.fit_one_cycle() = fits the data to classifier (input: how many cycles?, creates the weights)\n",
    "10. ConvLearner.save('name')  = saves the weights\n",
    "3. ConvLearner.unfreeze() = please train the whole model\n",
    "4. ConvLearner.load('name') = loads the previously saved model weights\n",
    "5. ConvLeaner.lr_find() = what is the fastest I can train the model at where it maintains accuracy\n",
    "6. ConvLearner.recorder.plot() = plots learning rate and loss\n",
    "7. ConvLearner.fit_one_cycle(2, max_lr = slice(1e-6, 1e-4) (passes a range of learning rates, train early layers at 1e-6 and later layers at 1e-4)\n",
    "\n",
    "\n",
    "##### CLASSIFICATIONINTERPRETATION FUNCTIONS\n",
    "1. ClassificationInterpretation.plot_top_losses(number) = shows the top errors, loss, label and prediction, one of the most useful things\n",
    "12. ClassificationInterpretation.plot_confusion_matrix(figsize=(12,12), dpi=60) = for every actual dog or cat, how many times was it predicted, hard to see if very accurate so instead use...\n",
    "13. ClassificationInterpretation.most_confused(min_val=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **KEY POINTS**\n",
    "1. **Transfer Learning:** Takes a model that already knows how to do one thing really well and tries to get it to do our thing really well. Can trained models 1/100 or less of the time usually needed with 1/100 or less data.\n",
    "2. **Overfitting:** Learning to recognize particular photos\n",
    "3. **Validation set:** set our model never got to see\n",
    "4. **Unfreezing**\n",
    "5. **Fine Tuning** Make model better by fine tuning. We took a premade model with a lot of layers, added a few layers at the end and trained those. When we went ahead and trained the entire model, the error rate increased. \n",
    "6. **Ways to label data**: From a folder with the label names (databunch.from_folder), from a csv file with a label column (.from_csv), in a file name or path (from_name_re, or from_name_func), an array of labels (pass it in the imagedatabunch)\n",
    "\n",
    "#### **Q&A Notes**\n",
    "1. What if the labels are part of the path name? \n",
    "   Extract labels from names using regular expressions\n",
    "2. GPU has to provide the exact same instruction to everything at the same time and in order to do this quickly = make them all the same shape. Most use 224 x 224. This generally just works.\n",
    "3. Why use resnet?\n",
    "\n",
    "#### **ADVICE**\n",
    "1. Always take a look at your data and your labels\n",
    "2. Start with resnet34 since it trains faster and see if it's good enough\n",
    "3. Spend a lot of time looking at the fast.ai documentation\n",
    "4. Pick one project, do it really well\n",
    "5. We always need to know the inputs and outputs of our functions\n",
    "\n",
    "\n",
    "#### **PROCESS**\n",
    "1. Import (from fastai.vision import *, from fastai.metrics import error_rate)\n",
    "2. Create a path & extract all data (untar_data()\n",
    "3. \n",
    "\n",
    "#### **TO-DO**\n",
    "\n",
    "\n",
    "#### **READING & EXPLORING**\n",
    "1. envision - ai\n",
    "2. papers with code\n",
    "3. fastai_docs #can download repos\n",
    "4. https://regexr.com/ # visualize regular expressions\n",
    "\n",
    "#### **QUESTIONS**\n",
    "1. What are path objects?\n",
    "2. Why is normalization important?\n",
    "3. What is one cycle?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **KEY NOTES**\n",
    "\n",
    "#### **OBJECTS**\n",
    "\n",
    "#### **FUNCTIONS**\n",
    "\n",
    "\n",
    "\n",
    "##### DATABUNCH FUNCTIONS\n",
    "\n",
    "\n",
    "##### CONVLEARNER FUNCTIONS\n",
    "\n",
    "\n",
    "##### CLASSIFICATIONINTERPRETATION FUNCTIONS\n",
    "\n",
    "\n",
    "\n",
    "#### **KEY POINTS**\n",
    "\n",
    "\n",
    "#### **Q&A Notes**\n",
    "\n",
    "#### **ADVICE**\n",
    "\n",
    "\n",
    "#### **PROCESS**\n",
    "\n",
    "\n",
    "#### **TO-DO**\n",
    "1. Production via web app\n",
    "\n",
    "\n",
    "#### **READING & EXPLORING**\n",
    "\n",
    "\n",
    "#### **QUESTIONS**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **KEY NOTES**\n",
    "\n",
    "#### **OBJECTS**\n",
    "\n",
    "#### **FUNCTIONS**\n",
    "\n",
    "\n",
    "#### **KEY POINTS**\n",
    "\n",
    "##### Types of models to build:\n",
    "1. Multi-label prediction - satellite imaging\n",
    "2. Every pixel in picture is colorcoded\n",
    "3. Object Detection dataset\n",
    "\n",
    "##### How to download kaggle datasets via python\n",
    "1. Kaggle API\n",
    "\n",
    "##### Data Block API\n",
    "1. \"The trickiest step previously in deep learning was processing data to a form where it would be ready to be put in a model\"\n",
    "2. We can use factory methods (premade databunch objects?) but sometimes we want flexibility\n",
    "3. Choices to make: where are the files? what structure? how do labels appear? how do you put out the validation set? how do we transform it?\n",
    "4. We have a datablock API which makes each of those decisions a separate method with their own parameters around how we set up our data. (SEE CODE BELOW)\n",
    "5. Class Dataset: an abstract class that represents a dataset and it holds two dunder functions: __getitem__ (allows us to index) and __len__ (allows us to get length).\n",
    "6. We need a few images at a time to train the model in parallel so we have to create a minibatch\n",
    "7. To create a minibatch we use another pytorch class called DataLoader: this takes a dataset and grab items at random creating a batch size of whatever you ask for and passes it to GPU to send it to model\n",
    "8. Then we use a fast.ai class called DataBunch which binds together a training data loader and a validation data loader (optional: test data loader) \n",
    "\n",
    "##### Mini-summaries\n",
    "1. **DataSet class**: Defines what the dataset needs to do (get item and length)\n",
    "2. **Data Loader**: Grabs individual items and combines them into a minibatch which is a few images we present to a model at a time that can be trained in parallel\n",
    "3. **DataBunch**: Binds together a training data loader and validation data loader. This is then sent to a learner.\n",
    "4. **Data Block API**: Helps us cutomize how to create a DatBunch by isolating underlying parts of that process into separate blocks like:\n",
    " - Where are the inputs\n",
    " - How to label\n",
    " - How to split the data into training and validation\n",
    " - What type of dataset \n",
    " - Possible transformations to apply\n",
    " - How to warp in dataloader to create a databunch\n",
    "\n",
    "##### How to handle unbalanced data\n",
    "1. Oversample\n",
    "2. Undersample \n",
    "3. https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR EXAMPLE\n",
    "#data = (ImageList.from_csv(path, 'train_v2.csv', folder='train-jpg', suffix='.jpg') #image files from a csv\n",
    "#        .split_by_rand_pct(0.2) #randomly spit out valid from 20% of the data\n",
    "#        .label_from_df(label_delim=' ') #labeled from df with a separator\n",
    "#        .transform(tfms, size=128) #\n",
    "#        .databunch()\n",
    "#        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Dunder methods: \n",
    "\n",
    "Dunder or magic methods in Python are the methods having two prefix and suffix underscores in the method name. Dunder here means “Double Under (Underscores)”. These are commonly used for operator overloading. Few examples for magic methods are: __init__, __add__, __len__, __repr__ \n",
    "\n",
    "In Python, special methods are a set of predefined methods you can use to enrich your classes. They are easy to recognize because they start and end with double underscores, for example __init__ or __str__.\n",
    "\n",
    "As it quickly became tiresome to say under-under-method-under-under Pythonistas adopted the term “dunder methods”, a short form of “double under.”\n",
    "\n",
    "These “dunders” or “special methods” in Python are also sometimes called “magic methods.” But using this terminology can make them seem more complicated than they really are—at the end of the day there’s nothing “magical” about them. You should treat these methods like a normal language feature.\n",
    "\n",
    "Dunder methods let you emulate the behavior of built-in types. For example, to get the length of a string you can call len('string'). But an empty class definition doesn’t support this behavior out of the box.\n",
    "\n",
    "1. https://dbader.org/blog/python-dunder-methods\n",
    "2. https://amontalenti.com/2013/04/11/python-double-under-double-wonder\n",
    "3. https://www.youtube.com/watch?v=RaDuYfqpxwM&feature=youtu.be\n",
    "4. **Object-Oriented Programmimg**: https://www.makeschool.com/academy/track/standalone/superhero-team-dueler/superhero-objects\n",
    "\n",
    "#### **Q&A Notes**\n",
    "\n",
    "#### **ADVICE**\n",
    "\n",
    "\n",
    "#### **PROCESS**\n",
    "\n",
    "\n",
    "#### **TO-DO**\n",
    "1. 9:35 - Create a web app for these applications to see if we understand the material\n",
    "\n",
    "#### **READING & EXPLORING**\n",
    "1. **Data block API:** https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4\n",
    "\n",
    "\n",
    "#### **QUESTIONS**\n",
    "1. How to improve AUC score for unbalanced data\n",
    "2. is there a way to use all of the smaller class and some of the bigger class to balance the data?\n",
    "3. How does the data block API work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    def __init__(self, name, breed, age):\n",
    "        self.name = name\n",
    "        self.breed = breed\n",
    "        self.age = age\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.name)\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def bark(self):\n",
    "        print('woof woof woof!')\n",
    "        \n",
    "    def growl(self):\n",
    "        print('grrrrrr!')\n",
    "    \n",
    "    def eat(self):\n",
    "        print('what does {} want?'.format(self.name))\n",
    "        food = input()\n",
    "        print('{} will have {}'.format(self.name,food))\n",
    "    \n",
    "    def walk(self):\n",
    "        print('where does {} want to walk?'.format(self.name))\n",
    "        location = input()\n",
    "        n = int(randint(1,100) % 2)\n",
    "        if n == 0:\n",
    "            print('{} can walk at {} today'.format(self.name,location))\n",
    "        if n ==1: \n",
    "            print('Sorry {}, probably not today'.format(self.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mocha\n"
     ]
    }
   ],
   "source": [
    "print(mocha.get_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda for the Day\n",
    "1. 6/18:\n",
    "    - **Manage unbalanced data** by under/oversampling methods\n",
    "    - Take **partial data** to train\n",
    "2. 6/23: \n",
    "    - Make a **dataset from the losses** and train model on those\n",
    "    - Understand Jeremy Howard's tips on **Learning Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning Rate\n",
    "1. **Before Unfreezing:** Find the lr with the steepest slope not at the bottom because you will slide down off quickly. If you choose the bottom value, it will send you up. So choose lr somewhere in the middle of steepest slope.\n",
    "2. **After Unfreezing:** Look for a point where it immediately shoots up and go back 5 or 10 steps. So if it shoots up at 1e-03, then choose 1e-04. Then you want to set lr to ***slice(10 steps before it immediately shoots up, lr before unfreezing divided by 5 or 10) = slice(1e-04, lr/5)***\n",
    "\n",
    "##### Transfer Learning\n",
    "To make a new databunch and train our learner with new data, all we need to do is run **learn.data = data** \n",
    "\n",
    "##### Saving\n",
    "- learn.save('stage-number-imagesize-architecture)\n",
    "- learn.save('stage-4-256-rn34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
